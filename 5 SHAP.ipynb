{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"SHAP.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1f_PQb5PnIu1OhVqJ6PYmhroUjbEjhKZ6","authorship_tag":"ABX9TyPYgnQWx18fqo5oy4mrRrd0"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"04mzFUyIWqk7","executionInfo":{"status":"ok","timestamp":1629284476390,"user_tz":-120,"elapsed":10976,"user":{"displayName":"Francesco Pasceri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi4P8_TVbJHrjb6Mlp8vyhnKCg8RJBB9HBNB75_ouE=s64","userId":"17848331632849336329"}},"outputId":"03b355e2-559c-4680-d656-8ea32c6d4044"},"source":["!pip install -q shap lime\n","!pip install -q transformers\n","!pip install -q sentence-transformers\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","%cd drive/MyDrive/Tesi\\ Magistrale\\ 204963/Progetto/\n"," \n","import warnings\n","warnings.filterwarnings(\"ignore\")"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","/content/drive/MyDrive/Tesi\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sHwCc-l_YYca","executionInfo":{"status":"ok","timestamp":1629284487627,"user_tz":-120,"elapsed":11246,"user":{"displayName":"Francesco Pasceri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi4P8_TVbJHrjb6Mlp8vyhnKCg8RJBB9HBNB75_ouE=s64","userId":"17848331632849336329"}},"outputId":"5c09d6c8-315f-4e56-ae1c-9ebf959d3b02"},"source":["import shap\n","\n","from math import ceil\n","\n","import torch\n","import numpy as np\n","import os\n","import pickle\n","import nltk\n","import pandas as pd\n","import tensorflow as tf\n","import matplotlib.pyplot as plt\n","#from matplotlib_venn_wordcloud import venn2_wordcloud\n","\n","from autoencoders.model import VAE, DAE, AAE, reparameterize\n","from autoencoders.vocab import Vocab\n","from autoencoders.noise import noisy\n","from autoencoders.utils import set_seed, strip_eos\n","from autoencoders.batchify import get_batches\n","from classifier.classifier_nn import build_nn\n","from nltk.corpus import stopwords\n","import lime.lime_text\n","from text_interpretable import *\n","from coherence import compute_explanations, compare_explanations, compute_score\n","\n","from sentence_transformers import SentenceTransformer, util\n","SentenceTransformer('stsb-roberta-large')\n","\n","nltk.download('stopwords')\n","nltk.download('averaged_perceptron_tagger')\n","nltk.download('wordnet')"],"execution_count":2,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n","[nltk_data]       date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"code","metadata":{"id":"y-CHyFfJYNXN","executionInfo":{"status":"ok","timestamp":1629284487629,"user_tz":-120,"elapsed":17,"user":{"displayName":"Francesco Pasceri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi4P8_TVbJHrjb6Mlp8vyhnKCg8RJBB9HBNB75_ouE=s64","userId":"17848331632849336329"}}},"source":["def load_sent(path):\n","    sents = []\n","    with open(path) as f:\n","        for line in f:\n","            sents.append(line)\n","    return sents"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UGLwqlpzXqgQ","executionInfo":{"status":"ok","timestamp":1629284490641,"user_tz":-120,"elapsed":3027,"user":{"displayName":"Francesco Pasceri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi4P8_TVbJHrjb6Mlp8vyhnKCg8RJBB9HBNB75_ouE=s64","userId":"17848331632849336329"}},"outputId":"633a6426-4604-43bd-f54c-990f1040b153"},"source":["data = 'yelp'\n","\n","if data == 'yelp':\n","  path = 'checkpoints/yelp/daae'\n","  dataset = 'data/yelp/sentiment'\n","  class_folder = 'classifier/yelp'\n","\n","  # Exctract data of interest\n","  # data_classes = data[(data['stars'] == 1) | (data['stars'] == 5)]\n","  # x = data_classes['text']\n","  # y = data_classes['stars'].where(data_classes['stars'] == 1, other=2) - 1\n","  sents = load_sent(os.path.join(dataset, '1000.pos'))\n","  num_example = 500 # SOLO TEST\n","  sents = sents[:num_example] # SOLO TEST\n","  n_pos = len(sents)\n","  sents.extend(load_sent(os.path.join(dataset, '1000.neg')))\n","  sents = sents[:2*num_example] # SOLO TEST\n","  y = np.ones(len(sents), dtype=int)\n","  y[n_pos:] = 0\n","\n","  vec = pickle.load(open(os.path.join(class_folder, 'vec_nn.pickle'), 'rb'))\n","  classifier = build_nn(len(vec.vocabulary_))\n","  classifier.load_weights(os.path.join(class_folder, 'model_dense.hdf5'))\n","  \n","  def classifier_fn(s):\n","    p = classifier.predict(vec.transform(s).toarray())\n","    return np.append(1-p, p, axis=1)\n","\n","  cv_d = pickle.load(open('mytest/yelp/new_CV.pickle','rb'))\n","  score_fabrizio_correzione = pickle.load(open('mytest/yelp/GOW_score.pickle','rb'))\n","\n","score_dict = dict(zip(cv_d.get_feature_names(), score_fabrizio_correzione))\n","\n","def score_parola(w):\n","  if w in score_dict:\n","    return score_dict[w]\n","  return 0.0\n","\n","print(\"INFO:\")\n","print(\"\\tsents \", type(sents), len(sents))\n","print(\"\\ty \", type(y), len(y))"],"execution_count":4,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n","Instructions for updating:\n","If using Keras pass *_constraint arguments to layers.\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","INFO:\n","\tsents  <class 'list'> 1000\n","\ty  <class 'numpy.ndarray'> 1000\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"mT6-D_zsaAmP","executionInfo":{"status":"ok","timestamp":1629284490643,"user_tz":-120,"elapsed":15,"user":{"displayName":"Francesco Pasceri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi4P8_TVbJHrjb6Mlp8vyhnKCg8RJBB9HBNB75_ouE=s64","userId":"17848331632849336329"}}},"source":["stop_words = pickle.load(open('stop_words_yelp.pickle', 'rb'))\n","lt = LemmaTokenizer(split_expression=r'\\W+', stop_words=stop_words)\n","tlt = LimeTextExplainerLatent()\n","lte = lime.lime_text.LimeTextExplainer()"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"_P-g4ceraHLh","executionInfo":{"status":"ok","timestamp":1629284490645,"user_tz":-120,"elapsed":15,"user":{"displayName":"Francesco Pasceri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi4P8_TVbJHrjb6Mlp8vyhnKCg8RJBB9HBNB75_ouE=s64","userId":"17848331632849336329"}}},"source":["def score_sim_parola(w, e1, e2):\n","  # compute similarity scores of two embeddings\n","  cosine_scores = util.pytorch_cos_sim(e1, e2)\n","  return abs(cosine_scores.item())\n","\n","def score_calc(w, e1, e2):\n","  sim = score_sim_parola(w, e1, e2)\n","  score = score_parola(w)\n","  if sim <= 0.3:\n","    return 2/3*score+sim\n","  if score <= 0.3:\n","    return score+2/3*sim\n","  return score+4/5*sim\n","\n","\n","def final_score(word, sentence, e1, e2, classifier_fn):\n","  \n","  '''\n","  Compute term relevance scores for a given sentence.\n","    word: single word or group of words which is a possible good explanation\n","    sentence: original sentence in lemmatized version \n","    e1: embedding of sentence\n","    e2: embedding of word\n","    classifier_fn: black-box prediction function\n","  '''\n","  sim = abs((util.pytorch_cos_sim(e1, e2)).item()) # vedo quanto è simile il termine nella frase\n","  score_word = score_parola(word) # se è presente nel cv principale e ha capacità di discernere\n","\n","  clf_bb =  np.argmax(classifier_fn([sentence]))\n","\n","  \n","  present = all(w in sentence for w in word.split()) # se la parola è contenuta nella frase\n","  similiar = sim >= 0.245\n","  distinguisher = score_word >= 0.3\n","\n","  important_absence, important_presence = False, False\n","  \n","  if present:\n","    a = ' '.join([x for x in sentence.split() if x not in word.split()]) # tolgo se presente\n","    important_absence = np.argmax(classifier_fn([a])) != clf_bb\n","  else:\n","    a = sentence + ' ' + word # la inserisco\n","    a = ' '.join(general_delete_rep(a.split())) # rimuovo doppioni\n","    important_presence = np.argmax(classifier_fn([a])) != clf_bb\n","\n","  if present:\n","    if important_absence:\n","      return sim+score_word+0.2\n","    else:\n","      if similiar:\n","        return sim+score_word\n","      else:\n","        if distinguisher:\n","          return score_word/1.5\n","        else:\n","          return 0.0\n","  else:\n","    if important_presence:\n","      return sim+score_word+0.2\n","    else:\n","      if similiar:\n","        return sim+score_word\n","      else:\n","        if distinguisher:\n","          return score_word/1.5\n","        else:\n","          return 0.0"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4j_p0V40aKoX","executionInfo":{"status":"ok","timestamp":1629284537971,"user_tz":-120,"elapsed":47340,"user":{"displayName":"Francesco Pasceri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi4P8_TVbJHrjb6Mlp8vyhnKCg8RJBB9HBNB75_ouE=s64","userId":"17848331632849336329"}},"outputId":"d5d1bc09-ae62-4954-e1cb-d2856d12b87a"},"source":["x_train = vec.transform( lt(sents) ).toarray()\n","print(x_train.shape)\n","\n","shap.explainers._deep.deep_tf.op_handlers[\"AddV2\"] = shap.explainers._deep.deep_tf.passthrough\n","\n","explainer = shap.DeepExplainer( classifier, x_train )\n","shap_values = explainer.shap_values( x_train )\n","\n","print(shap_values[0].shape)"],"execution_count":7,"outputs":[{"output_type":"stream","text":["(1000, 2065)\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/shap/explainers/tf_utils.py:28: The name tf.keras.backend.get_session is deprecated. Please use tf.compat.v1.keras.backend.get_session instead.\n","\n","(1000, 2065)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"rszpSfGcdQI7","executionInfo":{"status":"ok","timestamp":1629284783198,"user_tz":-120,"elapsed":245255,"user":{"displayName":"Francesco Pasceri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi4P8_TVbJHrjb6Mlp8vyhnKCg8RJBB9HBNB75_ouE=s64","userId":"17848331632849336329"}}},"source":["trf = SentenceTransformer('stsb-roberta-large')\n","\n","output = []\n","\n","for i in range(shap_values[0].shape[0]):\n","  ph = lt( [sents[i]] )[0]\n","  sentence_values = shap_values[0][i]\n","  \n","  top_4_index = sorted(range(len(sentence_values)), key=lambda i: sentence_values[i])[-4:]\n","  top_4_values = sorted( sentence_values )[-4:]\n","  words = [vec.get_feature_names()[ind] for ind in top_4_index]\n","\n","  #print(sents[i][:-1],'->',ph)\n","  #print(\"\\tWORDS \".ljust(40),\":\\t\", \"SHAP\".ljust(6), '\\t', \"SLIME\".ljust(6) )\n","\n","  e1 = trf.encode(ph, convert_to_tensor=True)\n","\n","  scores = []\n","\n","  for j in range(len(words)):\n","    w = words[j]\n","    e2 = trf.encode(w, convert_to_tensor=True)\n","    #print(\"\\t\"+w.ljust(40),\":\\t\", \"%.4f\" % top_4_values[j], \":\\t\", \"%.4f\" % final_score(w,ph,e1,e2,classifier_fn) )\n","    scores.append( final_score(w,ph,e1,e2,classifier_fn) )\n","  \n","  output.append(scores)\n","  #print()\n","\n","pickle.dump( output, open('../Confronto sLIME/shap_top_4.pickle','wb') )"],"execution_count":8,"outputs":[]}]}