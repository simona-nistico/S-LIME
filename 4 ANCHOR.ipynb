{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ANCHOR.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNmQP1QNqcA7akS2BN3lK+Q"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"A972rfJCsQ8K","executionInfo":{"status":"ok","timestamp":1629288539721,"user_tz":-120,"elapsed":67231,"user":{"displayName":"Francesco Pasceri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi4P8_TVbJHrjb6Mlp8vyhnKCg8RJBB9HBNB75_ouE=s64","userId":"17848331632849336329"}},"outputId":"2caab33b-9704-4f42-e174-54bbab2cba75"},"source":["!pip install -q anchor-exp\n","!python -m spacy download en_core_web_lg\n","!pip install -q transformers\n","!pip install -q sentence-transformers\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","%cd drive/MyDrive/Tesi\\ Magistrale\\ 204963/Progetto/\n"," \n","import warnings\n","warnings.filterwarnings(\"ignore\")"],"execution_count":10,"outputs":[{"output_type":"stream","text":["Collecting en_core_web_lg==2.2.5\n","  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.2.5/en_core_web_lg-2.2.5.tar.gz (827.9 MB)\n","\u001b[K     |████████████████████████████████| 827.9 MB 1.2 MB/s \n","\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_lg==2.2.5) (2.2.4)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.19.5)\n","Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.1.3)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (3.0.5)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (2.23.0)\n","Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (7.4.0)\n","Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (0.4.1)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (2.0.5)\n","Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (0.8.2)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (57.4.0)\n","Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.0)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.5)\n","Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.5)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (4.62.0)\n","Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (4.6.3)\n","Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.7.4.3)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.5.0)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (2021.5.30)\n","\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the model via spacy.load('en_core_web_lg')\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","[Errno 2] No such file or directory: 'drive/MyDrive/Tesi/'\n","/content/drive/MyDrive/Tesi\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cZY70aY6siNk","executionInfo":{"status":"ok","timestamp":1629288544093,"user_tz":-120,"elapsed":4383,"user":{"displayName":"Francesco Pasceri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi4P8_TVbJHrjb6Mlp8vyhnKCg8RJBB9HBNB75_ouE=s64","userId":"17848331632849336329"}},"outputId":"2c8aff73-d15d-4949-9d0c-78ffb06a2271"},"source":["import os\n","import os.path\n","import numpy as np\n","import sklearn\n","import sklearn.model_selection\n","import sklearn.linear_model\n","import sklearn.ensemble\n","import spacy\n","import sys\n","from sklearn.feature_extraction.text import CountVectorizer\n","from anchor import anchor_text\n","import time\n","\n","from math import ceil\n","\n","import torch\n","import pickle\n","import nltk\n","import pandas as pd\n","import tensorflow as tsf\n","import matplotlib.pyplot as plt\n","#from matplotlib_venn_wordcloud import venn2_wordcloud\n","\n","from autoencoders.model import VAE, DAE, AAE, reparameterize\n","from autoencoders.vocab import Vocab\n","from autoencoders.noise import noisy\n","from autoencoders.utils import set_seed, strip_eos\n","from autoencoders.batchify import get_batches\n","from classifier.classifier_nn import build_nn\n","from nltk.corpus import stopwords\n","import lime.lime_text\n","from text_interpretable import *\n","from coherence import compute_explanations, compare_explanations, compute_score\n","\n","from sentence_transformers import SentenceTransformer, util\n","SentenceTransformer('stsb-roberta-large')\n","\n","nltk.download('stopwords')\n","nltk.download('averaged_perceptron_tagger')\n","nltk.download('wordnet')"],"execution_count":11,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n","[nltk_data]       date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"code","metadata":{"id":"evoK16koszr5","executionInfo":{"status":"ok","timestamp":1629288544094,"user_tz":-120,"elapsed":12,"user":{"displayName":"Francesco Pasceri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi4P8_TVbJHrjb6Mlp8vyhnKCg8RJBB9HBNB75_ouE=s64","userId":"17848331632849336329"}}},"source":["def load_sent(path):\n","    sents = []\n","    with open(path) as f:\n","        for line in f:\n","            sents.append(line)\n","    return sents"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2KpFlQ-WuKS_","executionInfo":{"status":"ok","timestamp":1629288547389,"user_tz":-120,"elapsed":3304,"user":{"displayName":"Francesco Pasceri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi4P8_TVbJHrjb6Mlp8vyhnKCg8RJBB9HBNB75_ouE=s64","userId":"17848331632849336329"}},"outputId":"cd0c0ba8-3606-49af-a3c5-f54e878d0565"},"source":["data = 'yelp'\n","\n","if data == 'yelp':\n","  path = 'checkpoints/yelp/daae'\n","  dataset = 'data/yelp/sentiment'\n","  class_folder = 'classifier/yelp'\n","\n","  # Exctract data of interest\n","  # data_classes = data[(data['stars'] == 1) | (data['stars'] == 5)]\n","  # x = data_classes['text']\n","  # y = data_classes['stars'].where(data_classes['stars'] == 1, other=2) - 1\n","  sents = load_sent(os.path.join(dataset, '1000.pos'))\n","  num_example = 500 # SOLO \n","  sents = sents[:num_example] # SOLO TEST\n","  n_pos = len(sents)\n","  sents.extend(load_sent(os.path.join(dataset, '1000.neg')))\n","  sents = sents[:2*num_example] # SOLO TEST\n","  y = np.ones(len(sents), dtype=int)\n","  y[n_pos:] = 0\n","\n","  vec = pickle.load(open(os.path.join(class_folder, 'vec_nn.pickle'), 'rb'))\n","  classifier = build_nn(len(vec.vocabulary_))\n","  classifier.load_weights(os.path.join(class_folder, 'model_dense.hdf5'))\n","  \n","  def classifier_fn(s):\n","    p = classifier.predict(vec.transform(s).toarray())\n","    return np.append(1-p, p, axis=1)\n","\n","  cv_d = pickle.load(open('mytest/yelp/new_CV.pickle','rb'))\n","  score_fabrizio_correzione = pickle.load(open('mytest/yelp/GOW_score.pickle','rb'))\n","\n","score_dict = dict(zip(cv_d.get_feature_names(), score_fabrizio_correzione))\n","\n","def score_parola(w):\n","  if w in score_dict:\n","    return score_dict[w]\n","  return 0.0\n","\n","print(\"INFO:\")\n","print(\"\\tsents \", type(sents), len(sents))\n","print(\"\\ty \", type(y), len(y))"],"execution_count":13,"outputs":[{"output_type":"stream","text":["INFO:\n","\tsents  <class 'list'> 1000\n","\ty  <class 'numpy.ndarray'> 1000\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"xIVsnCpTuqNz","executionInfo":{"status":"ok","timestamp":1629288547391,"user_tz":-120,"elapsed":23,"user":{"displayName":"Francesco Pasceri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi4P8_TVbJHrjb6Mlp8vyhnKCg8RJBB9HBNB75_ouE=s64","userId":"17848331632849336329"}}},"source":["stop_words = pickle.load(open('stop_words_yelp.pickle', 'rb'))\n","lt = LemmaTokenizer(split_expression=r'\\W+', stop_words=stop_words)\n","tlt = LimeTextExplainerLatent()\n","lte = lime.lime_text.LimeTextExplainer()"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"id":"G9WfQ5m9uN5J","executionInfo":{"status":"ok","timestamp":1629288547392,"user_tz":-120,"elapsed":19,"user":{"displayName":"Francesco Pasceri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi4P8_TVbJHrjb6Mlp8vyhnKCg8RJBB9HBNB75_ouE=s64","userId":"17848331632849336329"}}},"source":["def score_sim_parola(w, e1, e2):\n","  # compute similarity scores of two embeddings\n","  cosine_scores = util.pytorch_cos_sim(e1, e2)\n","  return abs(cosine_scores.item())\n","\n","def score_calc(w, e1, e2):\n","  sim = score_sim_parola(w, e1, e2)\n","  score = score_parola(w)\n","  if sim <= 0.3:\n","    return 2/3*score+sim\n","  if score <= 0.3:\n","    return score+2/3*sim\n","  return score+4/5*sim\n","\n","\n","def final_score(word, sentence, e1, e2, classifier_fn):\n","  \n","  '''\n","  Compute term relevance scores for a given sentence.\n","    word: single word or group of words which is a possible good explanation\n","    sentence: original sentence in lemmatized version \n","    e1: embedding of sentence\n","    e2: embedding of word\n","    classifier_fn: black-box prediction function\n","  '''\n","  sim = abs((util.pytorch_cos_sim(e1, e2)).item()) # vedo quanto è simile il termine nella frase\n","  score_word = score_parola(word) # se è presente nel cv principale e ha capacità di discernere\n","\n","  clf_bb =  np.argmax(classifier_fn([sentence]))\n","\n","  \n","  present = all(w in sentence for w in word.split()) # se la parola è contenuta nella frase\n","  similiar = sim >= 0.245\n","  distinguisher = score_word >= 0.3\n","\n","  important_absence, important_presence = False, False\n","  \n","  if present:\n","    a = ' '.join([x for x in sentence.split() if x not in word.split()]) # tolgo se presente\n","    important_absence = np.argmax(classifier_fn([a])) != clf_bb\n","  else:\n","    a = sentence + ' ' + word # la inserisco\n","    a = ' '.join(general_delete_rep(a.split())) # rimuovo doppioni\n","    important_presence = np.argmax(classifier_fn([a])) != clf_bb\n","\n","  if present:\n","    if important_absence:\n","      return sim+score_word+0.2\n","    else:\n","      if similiar:\n","        return sim+score_word\n","      else:\n","        if distinguisher:\n","          return score_word/1.5\n","        else:\n","          return 0.0\n","  else:\n","    if important_presence:\n","      return sim+score_word+0.2\n","    else:\n","      if similiar:\n","        return sim+score_word\n","      else:\n","        if distinguisher:\n","          return score_word/1.5\n","        else:\n","          return 0.0"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"id":"ghBklnRqCVr6","executionInfo":{"status":"ok","timestamp":1629288547393,"user_tz":-120,"elapsed":17,"user":{"displayName":"Francesco Pasceri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi4P8_TVbJHrjb6Mlp8vyhnKCg8RJBB9HBNB75_ouE=s64","userId":"17848331632849336329"}}},"source":["def predict_fn(s):\n","  p = classifier.predict( vec.transform(s) )\n","  p = np.append(1-p, p, axis=1)\n","  return np.argmax( p, axis=1)"],"execution_count":16,"outputs":[]},{"cell_type":"code","metadata":{"id":"Kwpgq3oKuQke","executionInfo":{"status":"ok","timestamp":1629294515719,"user_tz":-120,"elapsed":3223613,"user":{"displayName":"Francesco Pasceri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi4P8_TVbJHrjb6Mlp8vyhnKCg8RJBB9HBNB75_ouE=s64","userId":"17848331632849336329"}}},"source":["nlp = spacy.load('en_core_web_sm')\n","explainer = anchor_text.AnchorText(nlp, predict_fn, use_unk_distribution=True)\n","class_names = ['negative', 'positive']\n","\n","d = {}\n","for i in range(len(sents)):\n","  ph = lt([sents[i]])\n","  #ph = [sents[i]]\n","  pred = class_names[predict_fn(ph)[0]]\n","  alternative =  class_names[1 - predict_fn(ph)[0]]\n","  exp = explainer.explain_instance( ph[0] , predict_fn, threshold=0.99, use_proba=False, batch_size=1)\n","  anchor = ' '.join(exp.names())\n","  d[i] = anchor"],"execution_count":20,"outputs":[]},{"cell_type":"code","metadata":{"id":"O4UGOwoBVibJ","executionInfo":{"status":"ok","timestamp":1629294639095,"user_tz":-120,"elapsed":123386,"user":{"displayName":"Francesco Pasceri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi4P8_TVbJHrjb6Mlp8vyhnKCg8RJBB9HBNB75_ouE=s64","userId":"17848331632849336329"}}},"source":["trf = SentenceTransformer('stsb-roberta-large')\n","output = []\n","\n","for i,anchor in d.items():\n","  if len(anchor) == 0:\n","    output.append(0.0)\n","  else:\n","    ph = lt( [sents[i]] )[0]  \n","    e1 = trf.encode(ph, convert_to_tensor=True)\n","    e2 = trf.encode(anchor, convert_to_tensor=True)\n","    output.append( final_score(anchor,ph,e1,e2,classifier_fn) )\n","\n","pickle.dump( output, open('../Confronto sLIME/anchor_result.pickle','wb') )"],"execution_count":21,"outputs":[]}]}