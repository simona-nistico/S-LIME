{"nbformat":4,"nbformat_minor":5,"metadata":{"accelerator":"GPU","colab":{"name":"1 Pre-processing and score computing.ipynb","provenance":[],"collapsed_sections":["subjective-distributor","1I8Eg_Y6cn0O","1jbht73cSxEq","ftAfqyFxacdn"]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.1"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UrQm3akp4X_W","executionInfo":{"status":"ok","timestamp":1628936875952,"user_tz":-120,"elapsed":11647,"user":{"displayName":"Francesco Pasceri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiCp3AccNC71QB-JHnhK1Pfh_beTF-jlvl8qlUJAA=s64","userId":"16512063719646044232"}},"outputId":"697058ac-94bb-4757-e536-3fddac5793e4"},"source":["!pip install -q lime\n","!pip install -q matplotlib_venn_wordcloud\n","\n","!pip install -q transformers\n","!pip install -q sentence-transformers\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","%cd drive/MyDrive/Tesi\\ Magistrale\\ 204963/Progetto"],"id":"UrQm3akp4X_W","execution_count":null,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","/content/drive/MyDrive/Tesi Magistrale 204963/Progetto\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"established-mandate","executionInfo":{"status":"ok","timestamp":1628936928280,"user_tz":-120,"elapsed":52337,"user":{"displayName":"Francesco Pasceri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiCp3AccNC71QB-JHnhK1Pfh_beTF-jlvl8qlUJAA=s64","userId":"16512063719646044232"}},"outputId":"77045a8f-1263-473e-d899-d908343386ce"},"source":["import numpy as np\n","import pickle\n","import nltk\n","import random\n","import pandas as pd\n","import os\n","import itertools\n","import torch\n","import math\n","import re\n","import scipy.sparse as sparse\n","from sklearn.feature_extraction.text import CountVectorizer\n","\n","from text_interpretable import LemmaTokenizer\n","from coherence import compute_score\n","from autoencoders.model import VAE, DAE, AAE, reparameterize\n","from autoencoders.vocab import Vocab\n","from autoencoders.noise import noisy\n","from autoencoders.utils import set_seed, strip_eos\n","from autoencoders.batchify import get_batches\n","from classifier.classifier_nn import build_nn\n","\n","from sklearn.model_selection import train_test_split\n","\n","nltk.download('averaged_perceptron_tagger')\n","nltk.download('wordnet')\n","nltk.download('stopwords')\n","\n","#from sentence_transformers import SentenceTransformer, util\n","#SentenceTransformer('stsb-roberta-large')\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\")"],"id":"established-mandate","execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n","[nltk_data]       date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"KCnqnlLm2Ahl"},"source":["def load_sent(path):\n","    sents = []\n","    with open(path) as f:\n","        for line in f:\n","          sents.append(line)\n","    return sents\n","\n","def get_model(path):\n","    ckpt = torch.load(path)\n","    train_args = ckpt['args']\n","    model = {'dae': DAE, 'vae': VAE, 'aae': AAE}[train_args.model_type](\n","        vocab, train_args).to(device)\n","    model.load_state_dict(ckpt['model'])\n","    model.flatten()\n","    model.eval()\n","    return model"],"id":"KCnqnlLm2Ahl","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"R0u07lKIxjyB"},"source":["dataset = 'yelp'"],"id":"R0u07lKIxjyB","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"scheduled-nancy","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1628936928678,"user_tz":-120,"elapsed":430,"user":{"displayName":"Francesco Pasceri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiCp3AccNC71QB-JHnhK1Pfh_beTF-jlvl8qlUJAA=s64","userId":"16512063719646044232"}},"outputId":"2f31d7a8-5000-4d76-8550-3bfdb492b790"},"source":["if dataset == 'yahoo':\n","  sents = pickle.load(open(\"data/yahoo_answers_csv/test_2_processed.pickle\",'rb'))\n","  y = pickle.load(open(\"data/yahoo_answers_csv/test_2_processed_y.pickle\",'rb'))\n","\n","  path = 'checkpoints/yahoo/daae'\n","\n","  cv_d = pickle.load(open(\"tests/yahoo/cv_d.pickle\",'rb'))\n","  score_slime = pickle.load(open('tests/yahoo/score_fabrizio_correzione.pickle', 'rb'))\n","\n","  sents_vocab = sents.copy()\n","\n","if dataset == 'yelp':\n","  sents = load_sent(os.path.join('data/yelp/sentiment/1000.pos'))\n","  n_pos = len(sents)\n","  sents.extend(load_sent(os.path.join('data/yelp/sentiment/1000.neg')))\n","  y = np.ones(len(sents), dtype=int)\n","  y[n_pos:] = 0\n","\n","  sents_vocab = np.append(sents,load_sent(os.path.join('data/yelp/train.txt')),axis=0)\n","\n","  path = 'checkpoints/yelp/daae'\n","\n","  cv_d = pickle.load(open(\"tests/tree/model_2/cv_d.pickle\",'rb'))\n","  score_slime = pickle.load(open('tests/tree/model_2/score_fabrizio_distance_2.pickle', 'rb'))\n","\n","# ALTRO DATASET #\n","if dataset == 'twitter':\n","  sents = load_sent( os.path.join('data/twitter/twitter_pos.txt') )\n","  n_pos = len(sents)\n","  sents.extend(load_sent(os.path.join('data/twitter/twitter_neg.txt')))\n","  y = np.ones(len(sents), dtype=int)\n","  y[n_pos:] = 0\n","\n","  sents_vocab = load_sent(os.path.join('data/twitter/twitter_train.txt'))\n","  \n","\n","print(\"sents_vocab =\", len(sents_vocab))\n","print(\"sents len =\",len(sents))\n","print(\"y len =\",len(y))"],"id":"scheduled-nancy","execution_count":null,"outputs":[{"output_type":"stream","text":["sents_vocab = 202000\n","sents len = 2000\n","y len = 2000\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"absolute-conservation"},"source":["Lemmatizer e rimozione stopwords"],"id":"absolute-conservation"},{"cell_type":"code","metadata":{"id":"plastic-president"},"source":["stopwords = pickle.load(open(\"stop_words_yelp.pickle\",'rb'))\n","\n","lt = LemmaTokenizer(split_expression=r'\\W+', stop_words=stopwords)\n","\n","data_lt = lt(sents_vocab)"],"id":"plastic-president","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"thermal-society","scrolled":true,"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1628527990010,"user_tz":-120,"elapsed":28,"user":{"displayName":"Francesco Pasceri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiCp3AccNC71QB-JHnhK1Pfh_beTF-jlvl8qlUJAA=s64","userId":"16512063719646044232"}},"outputId":"d10fb836-6a23-4795-eb08-b5413542a9a2"},"source":["for i in range(4):\n","    n = random.randint(a=0, b=len(data_lt))\n","    print(n,')',sents_vocab[n][:-1],'===>', data_lt[n])"],"id":"thermal-society","execution_count":null,"outputs":[{"output_type":"stream","text":["2088 ) 'before long , the film starts playing like general hospital crossed with a saturday night live spoof of dog day afternoon . ===> long film start play like general hospital cross saturday night live spoof dog day afternoon\n","7171 ) \"this movie may not have the highest production values you've ever seen , but it's the work of an artist , one whose view of america , history and the awkwardness of human life is generous and deep . ===> movie may not have high production value ve ever see but it work an artist one whose view america history awkwardness human life generous deep\n","1345 ) '[stevens is] so stoked to make an important film about human infidelity and happenstance that he tosses a kitchen sink onto a story already overladen with plot conceits . ===> stevens stoke make an important film human infidelity happenstance he toss kitchen sink onto story already overladen plot conceit\n","5658 ) 'as if to prove a female director can make a movie with no soft edges , kathryn bigelow offers no sugar-coating or interludes of lightness . her film is unrelentingly claustrophobic and unpleasant . ===> prove female director can make movie no soft edge kathryn bigelow offer no sugar coat interlude lightness film unrelentingly claustrophobic unpleasant\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"subjective-distributor"},"source":["CountVectorizer Single Word\n","==="],"id":"subjective-distributor"},{"cell_type":"code","metadata":{"id":"occupied-butterfly"},"source":["my_cv = CountVectorizer(binary=True)\n","my_cv.fit(data_lt)\n","\n","#data_cv = my_cv.transform(lt(sents)) \n","data_cv = my_cv.transform(data_lt)\n","\n","print(\"my vocab size =>\",len(my_cv.vocabulary_))\n","print(\"vocab size slime =>\",len(cv_d.vocabulary_))\n","\n","print(data_cv.shape)"],"id":"occupied-butterfly","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cly6XmxnXie7"},"source":["# DIFFERENZE TRA VOCAB MIO E VOCAB SLIME\n","diff = (set(my_cv.vocabulary_))-set(cv_d.vocabulary_)\n","print('num differences :',len(diff))"],"id":"cly6XmxnXie7","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"miniature-paper"},"source":["Batch splitting and score computing"],"id":"miniature-paper"},{"cell_type":"code","metadata":{"id":"0bsyEwOYXJZ0"},"source":["class_folder = 'classifier/'+dataset+'/'\n","\n","if dataset == 'yahoo':\n","  mnb = pickle.load(open(os.path.join(class_folder, 'mnb.pickle'), 'rb'))\n","  vec = pickle.load(open(os.path.join(class_folder, 'vec.pickle'), 'rb'))\n","  classifier_fn = lambda s: mnb.predict_proba(vec.transform(s))\n","\n","if dataset == 'yelp':\n","  vocab = Vocab(os.path.join(path, 'vocab.txt'))\n","\n","  cuda = torch.cuda.is_available()\n","  device = torch.device(\"cuda\" if cuda else \"cpu\")\n","  model = get_model(os.path.join(path, 'model.pt'))\n","\n","  vec = pickle.load(open(os.path.join(class_folder, 'vec_nn.pickle'), 'rb'))\n","  classifier = build_nn(len(vec.vocabulary_))\n","  classifier.load_weights(os.path.join(class_folder, 'model_dense.hdf5'))\n","  #classifier_fn = lambda s: classifier.predict(vec.transform(s).toarray())\n","\n","  def classifier_fn(s):\n","    p = classifier.predict(vec.transform(s).toarray())\n","    return np.append(1-p, p, axis=1) # prob di essere neg, prob di essere pos\n","\n","print(dataset)\n","pre_classes = classifier_fn(sents_vocab)\n","pre_classes = np.where(pre_classes[:,0]>=0.5, 0, 1)"],"id":"0bsyEwOYXJZ0","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gal3UxfWVowg"},"source":["from math import ceil\n","\n","batch_dim = 200\n","\n","sum_pos = np.zeros(len(cv_d.get_feature_names()))\n","sum_pos = sum_pos.reshape((1, sum_pos.shape[0]))\n","sum_tot = np.zeros(len(cv_d.get_feature_names()))\n","sum_tot = sum_tot.reshape((1, sum_tot.shape[0]))\n","\n","for i in range(ceil(data_cv.shape[0]/batch_dim)):\n","  sum_tot += np.sum( data_cv[i*batch_dim : (i+1)*batch_dim].todense(), axis=0 )\n","  sum_pos += np.sum( pre_classes[i*batch_dim : (i+1)*batch_dim] * data_cv[i*batch_dim : (i+1)*batch_dim].todense(), axis=0 )\n","\n","sum_pos = np.squeeze(sum_pos)\n","sum_tot = np.squeeze(sum_tot)\n","\n","print(sum_tot.shape)\n","print(sum_pos.shape)"],"id":"gal3UxfWVowg","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fossil-logan"},"source":["score = compute_score(sum_pos=sum_pos, sum_tot=sum_tot)\n","print(\"my score\",score.shape)\n","print(\"slime score\",score_slime.shape)\n","print()\n","print('my score sum',np.sum(score))\n","print('slime score sum',np.sum(score_slime))"],"id":"fossil-logan","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"proof-preservation"},"source":["df = pd.DataFrame([score, score_slime], index=['My_score', 'SLIME_score'] )\n","df.columns = cv_d.get_feature_names()\n","df"],"id":"proof-preservation","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mk0iO1dpmOUx"},"source":["with open(\"mytest/\"+dataset+\"/vocab_score\",'wb') as f:\n","  pickle.dump(score, f)\n","with open(\"mytest/\"+dataset+\"/my_CV\",'wb') as f:\n","  pickle.dump(my_cv, f)"],"id":"mk0iO1dpmOUx","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1I8Eg_Y6cn0O"},"source":["YELP : CountVectorizer N-Gram\n","==="],"id":"1I8Eg_Y6cn0O"},{"cell_type":"markdown","metadata":{"id":"liSZWZz00uMo"},"source":["Definisci modello e calcola occorrenze\n","---"],"id":"liSZWZz00uMo"},{"cell_type":"code","metadata":{"id":"VjzA16b-cuwe","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1628936792357,"user_tz":-120,"elapsed":5539,"user":{"displayName":"Francesco Pasceri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiCp3AccNC71QB-JHnhK1Pfh_beTF-jlvl8qlUJAA=s64","userId":"16512063719646044232"}},"outputId":"553cddc5-d5b9-4467-a243-ad447ce67d73"},"source":["new_CV = CountVectorizer(ngram_range=(1,3), binary=True)\n","data_cv = new_CV.fit_transform(data_lt)\n","len(new_CV.vocabulary_)\n","\n","#pickle.dump( new_CV, open('mytest/'+dataset+'/new_CV.pickle','wb') )"],"id":"VjzA16b-cuwe","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["821328"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"code","metadata":{"id":"MlKeR8-5u3uv","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1628936964263,"user_tz":-120,"elapsed":35589,"user":{"displayName":"Francesco Pasceri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiCp3AccNC71QB-JHnhK1Pfh_beTF-jlvl8qlUJAA=s64","userId":"16512063719646044232"}},"outputId":"845df2ba-1885-42e8-9f6b-601006be9fe0"},"source":["class_folder = 'classifier/'+dataset+'/'\n","\n","if dataset == 'yahoo':\n","  mnb = pickle.load(open(os.path.join(class_folder, 'mnb.pickle'), 'rb'))\n","  vec = pickle.load(open(os.path.join(class_folder, 'vec.pickle'), 'rb'))\n","  classifier_fn = lambda s: mnb.predict_proba(vec.transform(s))\n","\n","if dataset == 'yelp':\n","  vocab = Vocab(os.path.join(path, 'vocab.txt'))\n","\n","  cuda = torch.cuda.is_available()\n","  device = torch.device(\"cuda\" if cuda else \"cpu\")\n","  model = get_model(os.path.join(path, 'model.pt'))\n","\n","  vec = pickle.load(open(os.path.join(class_folder, 'vec_nn.pickle'), 'rb'))\n","  classifier = build_nn(len(vec.vocabulary_))\n","  classifier.load_weights(os.path.join(class_folder, 'model_dense.hdf5'))\n","  #classifier_fn = lambda s: classifier.predict(vec.transform(s).toarray())\n","\n","  def classifier_fn(s):\n","    p = classifier.predict(vec.transform(s).toarray())\n","    return np.append(1-p, p, axis=1) # prob di essere neg, prob di essere pos\n","\n","print(dataset)\n","pre_classes = classifier_fn(sents_vocab)\n","pre_classes = np.where(pre_classes[:,0]>=0.5, 0, 1)"],"id":"MlKeR8-5u3uv","execution_count":null,"outputs":[{"output_type":"stream","text":["yelp\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"zs9_L2j3fpRF"},"source":["from math import ceil\n","\n","batch_dim = 1000\n","\n","gow_sum_pos = np.zeros(len(new_CV.get_feature_names()))\n","gow_sum_pos = gow_sum_pos.reshape((1, gow_sum_pos.shape[0]))\n","gow_sum_tot = np.zeros(len(new_CV.get_feature_names()))\n","gow_sum_tot = gow_sum_tot.reshape((1, gow_sum_tot.shape[0]))\n","\n","for i in range(ceil(data_cv.shape[0]/batch_dim)):\n","  gow_sum_tot += np.sum( data_cv[i*batch_dim : (i+1)*batch_dim].todense(), axis=0 )\n","  gow_sum_pos += np.sum( pre_classes[i*batch_dim : (i+1)*batch_dim] * data_cv[i*batch_dim : (i+1)*batch_dim].todense(), axis=0 )\n","\n","gow_sum_pos = np.squeeze(gow_sum_pos)\n","gow_sum_tot = np.squeeze(gow_sum_tot)\n","\n","print(gow_sum_tot.shape)\n","print(gow_sum_pos.shape)"],"id":"zs9_L2j3fpRF","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HRE63oVv2x__"},"source":["Calcolo score\n","---"],"id":"HRE63oVv2x__"},{"cell_type":"code","metadata":{"id":"WsrkRshOgNgf"},"source":["gow_score = compute_score(sum_pos=gow_sum_pos, sum_tot=gow_sum_tot)\n","with open('mytest/'+dataset+'/GOW_score.pickle','wb') as f:\n","  pickle.dump(gow_score,f)\n","  \n","df = pd.DataFrame([gow_score], index=['My_score'] )\n","df.columns = new_CV.get_feature_names()\n","df_copy = df.copy()\n","df_copy = df_copy.max().sort_values(ascending=False)\n","df_copy[:50]"],"id":"WsrkRshOgNgf","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1jbht73cSxEq"},"source":["YAHOO: CountVectorized N-gram\n","===="],"id":"1jbht73cSxEq"},{"cell_type":"markdown","metadata":{"id":"C4enyIpxS6g2"},"source":["Definisci modello e calcola occorrenze\n","---"],"id":"C4enyIpxS6g2"},{"cell_type":"code","metadata":{"id":"yZo-Zc8JTDVK"},"source":["new_CV = CountVectorizer(ngram_range=(1,3), binary=True)\n","data_cv = new_CV.fit_transform(data_lt)\n","len(new_CV.vocabulary_)\n","\n","#pickle.dump( new_CV, open('mytest/'+dataset+'/new_CV.pickle','wb') )"],"id":"yZo-Zc8JTDVK","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JJguwXmqTF8v"},"source":["class_folder = 'classifier/'+dataset+'/'\n","\n","if dataset == 'yahoo':\n","  mnb = pickle.load(open(os.path.join(class_folder, 'mnb.pickle'), 'rb'))\n","  vec = pickle.load(open(os.path.join(class_folder, 'vec.pickle'), 'rb'))\n","  classifier_fn = lambda s: mnb.predict_proba(vec.transform(s))\n","\n","if dataset == 'yelp':\n","  vocab = Vocab(os.path.join(path, 'vocab.txt'))\n","\n","  cuda = torch.cuda.is_available()\n","  device = torch.device(\"cuda\" if cuda else \"cpu\")\n","  model = get_model(os.path.join(path, 'model.pt'))\n","\n","  vec = pickle.load(open(os.path.join(class_folder, 'vec_nn.pickle'), 'rb'))\n","  classifier = build_nn(len(vec.vocabulary_))\n","  classifier.load_weights(os.path.join(class_folder, 'model_dense.hdf5'))\n","  #classifier_fn = lambda s: classifier.predict(vec.transform(s).toarray())\n","\n","  def classifier_fn(s):\n","    p = classifier.predict(vec.transform(s).toarray())\n","    return np.append(1-p, p, axis=1) # prob di essere neg, prob di essere pos\n","\n","print(dataset)\n","pre_classes = classifier_fn(sents_vocab)\n","pre_classes = np.where(pre_classes[:,0]>=0.5, 0, 1)"],"id":"JJguwXmqTF8v","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ofmp9K9BTLcE"},"source":["from math import ceil\n","\n","batch_dim = 1000\n","\n","gow_sum_pos = np.zeros(len(new_CV.get_feature_names()))\n","gow_sum_pos = gow_sum_pos.reshape((1, gow_sum_pos.shape[0]))\n","gow_sum_tot = np.zeros(len(new_CV.get_feature_names()))\n","gow_sum_tot = gow_sum_tot.reshape((1, gow_sum_tot.shape[0]))\n","\n","for i in range(ceil(data_cv.shape[0]/batch_dim)):\n","  gow_sum_tot += np.sum( data_cv[i*batch_dim : (i+1)*batch_dim].todense(), axis=0 )\n","  gow_sum_pos += np.sum( pre_classes[i*batch_dim : (i+1)*batch_dim] * data_cv[i*batch_dim : (i+1)*batch_dim].todense(), axis=0 )\n","\n","gow_sum_pos = np.squeeze(gow_sum_pos)\n","gow_sum_tot = np.squeeze(gow_sum_tot)\n","\n","print(gow_sum_tot.shape)\n","print(gow_sum_pos.shape)"],"id":"ofmp9K9BTLcE","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9kfVx0jLS-d3"},"source":["Calcolo score\n","---"],"id":"9kfVx0jLS-d3"},{"cell_type":"code","metadata":{"id":"tkLZeo_jTOVk"},"source":["gow_score = compute_score(sum_pos=gow_sum_pos, sum_tot=gow_sum_tot)\n","with open('mytest/'+dataset+'/GOW_score.pickle','wb') as f:\n","  pickle.dump(gow_score,f)\n","  \n","df = pd.DataFrame([gow_score], index=['My_score'] )\n","df.columns = new_CV.get_feature_names()\n","df_copy = df.copy()\n","df_copy = df_copy.max().sort_values(ascending=False)\n","df_copy[:50]"],"id":"tkLZeo_jTOVk","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ftAfqyFxacdn"},"source":["TWITTER: CountVectorized N-gram\n","==="],"id":"ftAfqyFxacdn"},{"cell_type":"code","metadata":{"id":"iswdzIt9jwWE"},"source":["new_CV = CountVectorizer(ngram_range=(1,3), binary=True, max_features=820000)\n","data_cv = new_CV.fit_transform(data_lt)\n","len(new_CV.vocabulary_)\n","\n","pickle.dump( new_CV, open('mytest/'+dataset+'/new_CV.pickle','wb') )"],"id":"iswdzIt9jwWE","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kRHLSooXj3g0","executionInfo":{"status":"ok","timestamp":1628528090473,"user_tz":-120,"elapsed":10309,"user":{"displayName":"Francesco Pasceri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiCp3AccNC71QB-JHnhK1Pfh_beTF-jlvl8qlUJAA=s64","userId":"16512063719646044232"}},"outputId":"513a8295-8859-42fe-9acf-0ddee3b7ea75"},"source":["class_folder = 'classifier/'+dataset+'/'\n","\n","if dataset == 'twitter':\n","  vec = pickle.load(open(os.path.join(class_folder, 'vec_nn.pickle'), 'rb'))\n","  classifier = build_nn(len(vec.vocabulary_))\n","  try:\n","    classifier.load_weights(os.path.join(class_folder, 'model_dense.hdf5'))\n","  except:\n","    classifier.load_weights(os.path.join(class_folder, 'model_dense.hdf5'),'rb')\n","  #classifier_fn = lambda s: classifier.predict(vec.transform(s).toarray())\n","\n","def classifier_fn(s):\n","    chunks = [s[x:x+500] for x in range(0, len(s), 500)]\n","    result = []\n","    for chunk in chunks:\n","      p = classifier.predict(vec.transform(chunk).toarray())\n","      result.extend( np.append(1-p,p, axis=1) )\n","    del chunk\n","    del chunks\n","    return np.asarray(result)\n","\n","print(dataset)\n","pre_classes = classifier_fn(sents_vocab)\n","#pre_classes = np.where(pre_classes[:,0]>=0.5, 0, 1)\n","pre_classes = np.squeeze(pre_classes)\n","pre_classes = np.argmax(pre_classes, axis=1)"],"id":"kRHLSooXj3g0","execution_count":null,"outputs":[{"output_type":"stream","text":["movie\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"t3T7AM4_pT3J","executionInfo":{"status":"ok","timestamp":1628528112349,"user_tz":-120,"elapsed":21878,"user":{"displayName":"Francesco Pasceri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiCp3AccNC71QB-JHnhK1Pfh_beTF-jlvl8qlUJAA=s64","userId":"16512063719646044232"}},"outputId":"721796e2-f3e5-42f4-c82e-d9fb345e8884"},"source":["from math import ceil\n","\n","batch_dim = 712\n","\n","gow_sum_pos = np.zeros(len(new_CV.get_feature_names()))\n","gow_sum_pos = gow_sum_pos.reshape((1, gow_sum_pos.shape[0]))\n","gow_sum_tot = np.zeros(len(new_CV.get_feature_names()))\n","gow_sum_tot = gow_sum_tot.reshape((1, gow_sum_tot.shape[0]))\n","\n","for i in range(ceil(data_cv.shape[0]/batch_dim)):\n","  gow_sum_tot += np.sum( data_cv[i*batch_dim : (i+1)*batch_dim].todense(), axis=0 )\n","  gow_sum_pos += np.sum( pre_classes[i*batch_dim : (i+1)*batch_dim] * data_cv[i*batch_dim : (i+1)*batch_dim].todense(), axis=0 )\n","\n","gow_sum_pos = np.squeeze(gow_sum_pos)\n","gow_sum_tot = np.squeeze(gow_sum_tot)\n","\n","print(gow_sum_tot.shape)\n","print(gow_sum_pos.shape)"],"id":"t3T7AM4_pT3J","execution_count":null,"outputs":[{"output_type":"stream","text":["(209268,)\n","(209268,)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":162},"id":"nCuyzurdpZ6X","executionInfo":{"status":"ok","timestamp":1628528124180,"user_tz":-120,"elapsed":11866,"user":{"displayName":"Francesco Pasceri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiCp3AccNC71QB-JHnhK1Pfh_beTF-jlvl8qlUJAA=s64","userId":"16512063719646044232"}},"outputId":"cc1d0cfd-4f95-4287-87a3-aaa5a5498d91"},"source":["gow_score = compute_score(sum_pos=gow_sum_pos, sum_tot=gow_sum_tot)\n","with open('mytest/'+dataset+'/GOW_score.pickle','wb') as f:\n","  pickle.dump(gow_score,f)\n","  \n","df = pd.DataFrame([gow_score], index=['My_score'] )\n","df.columns = new_CV.get_feature_names()\n","df_copy = df.copy()\n","df_copy = df_copy.max().sort_values(ascending=False)\n","df_copy[:50]"],"id":"nCuyzurdpZ6X","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>_num_</th>\n","      <th>_num_ _num_</th>\n","      <th>_num_ _num_ _num_</th>\n","      <th>_num_ _num_ _num_st</th>\n","      <th>_num_ _num_ an</th>\n","      <th>_num_ _num_ but</th>\n","      <th>_num_ _num_ film</th>\n","      <th>_num_ _num_ hour</th>\n","      <th>_num_ _num_ league</th>\n","      <th>_num_ _num_ million</th>\n","      <th>_num_ _num_ minute</th>\n","      <th>_num_ _num_ new</th>\n","      <th>_num_ _num_ period</th>\n","      <th>_num_ _num_ philosophical</th>\n","      <th>_num_ _num_ powerpuff</th>\n","      <th>_num_ _num_ screen</th>\n","      <th>_num_ _num_ terrorist</th>\n","      <th>_num_ _num_ time</th>\n","      <th>_num_ _num_st</th>\n","      <th>_num_ _num_st century</th>\n","      <th>_num_ _num_ths</th>\n","      <th>_num_ _num_ths it</th>\n","      <th>_num_ actor</th>\n","      <th>_num_ actor appear</th>\n","      <th>_num_ aim</th>\n","      <th>_num_ aim specifically</th>\n","      <th>_num_ alien</th>\n","      <th>_num_ alien plucky</th>\n","      <th>_num_ also</th>\n","      <th>_num_ also happens</th>\n","      <th>_num_ alternative</th>\n","      <th>_num_ alternative housing</th>\n","      <th>_num_ an</th>\n","      <th>_num_ an antique</th>\n","      <th>_num_ an uneasy</th>\n","      <th>_num_ analyze</th>\n","      <th>_num_ analyze promise</th>\n","      <th>_num_ animation</th>\n","      <th>_num_ animation wild</th>\n","      <th>_num_ armenia</th>\n","      <th>...</th>\n","      <th>zone arm</th>\n","      <th>zone arm nothing</th>\n","      <th>zone attempt</th>\n","      <th>zone attempt be</th>\n","      <th>zone bale</th>\n","      <th>zone bale reduce</th>\n","      <th>zone be</th>\n","      <th>zone be laud</th>\n","      <th>zone but</th>\n","      <th>zone but it</th>\n","      <th>zone episode</th>\n","      <th>zone give</th>\n","      <th>zone give voice</th>\n","      <th>zone have</th>\n","      <th>zone have center</th>\n","      <th>zone honest</th>\n","      <th>zone honest enough</th>\n","      <th>zone left</th>\n","      <th>zone left me</th>\n","      <th>zone offer</th>\n","      <th>zone offer win</th>\n","      <th>zone ordinance</th>\n","      <th>zone ordinance protect</th>\n","      <th>zone sympathize</th>\n","      <th>zone sympathize terrorist</th>\n","      <th>zoolander</th>\n","      <th>zoolander thought</th>\n","      <th>zoolander thought rather</th>\n","      <th>zoom</th>\n","      <th>zoom it</th>\n","      <th>zoom it actually</th>\n","      <th>zucker</th>\n","      <th>zucker brother</th>\n","      <th>zucker brother abraham</th>\n","      <th>zwick</th>\n","      <th>zwick heartfelt</th>\n","      <th>zwick heartfelt hilarious</th>\n","      <th>zwick it</th>\n","      <th>zwick it sitcom</th>\n","      <th>zzzzzzzzz</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>My_score</th>\n","      <td>0.13423</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.066987</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>1 rows × 209268 columns</p>\n","</div>"],"text/plain":["            _num_  _num_ _num_  ...  zwick it sitcom  zzzzzzzzz\n","My_score  0.13423          0.0  ...              0.0        0.0\n","\n","[1 rows x 209268 columns]"]},"metadata":{"tags":[]},"execution_count":14}]}]}